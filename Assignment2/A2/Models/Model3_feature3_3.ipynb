{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model3_feature3_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKhhaHUvYgoa",
        "colab_type": "text"
      },
      "source": [
        "# Named Entity Recognition + 3 bi-lstm + 2 self-attention + 3 features\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xY4scQ1RUve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwUrZvQakO2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "id = '1m2je3m7MiyHJynZv8bUtmUYWUJy7vCTl'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('train.csv')\n",
        "\n",
        "id = '1Cm_SL9JHgJ6_1qh6FeuOpUFi_-lbtQw3'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('val.csv')\n",
        "\n",
        "id = '18Av4rRPwnlCdF2V4wXKioYlPNy7DgzFJ'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('test.csv')\n",
        "\n",
        "import pandas as pd\n",
        "df_train = pd.read_csv(\"train.csv\")\n",
        "df_val = pd.read_csv(\"val.csv\")\n",
        "df_test = pd.read_csv(\"test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLtRDEwSCMcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the files of the PoS tags for the dataset\n",
        "# PoS tags are used as one of the features \n",
        "\n",
        "id = '1UmNHdUZxjfcuIzCcAKuBvfBXdSWFv47i'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('train.txt')\n",
        "\n",
        "id = '11bZIh5V9m2nZJ5s5xQ_gxHEHkAEhV8eQ'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('validation.txt')\n",
        "\n",
        "id = '1V-LQuJWT62aCytYuhZuaxvICsqiF1rdK'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('test.txt')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK73yG9zfHbt",
        "colab_type": "code",
        "outputId": "97195b6f-44f5-4d00-a033-c268dff49291",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# create a list of Sentence and a list of NER\n",
        "\n",
        "sentences_train = df_train['Sentence'].tolist()\n",
        "ner_train = df_train['NER'].tolist()\n",
        "\n",
        "# split each sentence by \" \"\n",
        "sentence_train_split = []\n",
        "ner_train_split = []\n",
        "for each_sentence in sentences_train:\n",
        "    each_sentence = each_sentence.split(\" \")\n",
        "    sentence_train_split.append(each_sentence)\n",
        "\n",
        "for each_ner in ner_train:\n",
        "    each_ner = each_ner.split(\" \")\n",
        "    ner_train_split.append(each_ner)\n",
        "print(sentence_train_split[1][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNInRtB7PDia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences_val = df_val['Sentence'].tolist()\n",
        "ner_val = df_val['NER'].tolist()\n",
        "\n",
        "# split each sentence by \" \"\n",
        "sentence_val_split = []\n",
        "ner_val_split = []\n",
        "for each_sentence in sentences_val:\n",
        "    each_sentence = each_sentence.split(\" \")\n",
        "    sentence_val_split.append(each_sentence)\n",
        "\n",
        "for each_ner in ner_val:\n",
        "    each_ner = each_ner.split(\" \")\n",
        "    ner_val_split.append(each_ner)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjohNx5IvRVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a list of Sentence\n",
        "\n",
        "sentences_test = df_test['Sentence'].tolist()\n",
        "\n",
        "# split each sentence by \" \"\n",
        "sentence_test_split = []\n",
        "\n",
        "for each_sentence in sentences_test:\n",
        "    each_sentence = each_sentence.split(\" \")\n",
        "    sentence_test_split.append(each_sentence)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S049P4HVDJTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(file_name, n_sample):\n",
        "    f = open(file_name)\n",
        "    documents = f.readlines()\n",
        "    pos_data = []\n",
        "    pos = []\n",
        "    for i in documents:\n",
        "        if i == '\\n':   \n",
        "            pos_data.append(pos)\n",
        "            pos = []\n",
        "        else:    \n",
        "            pos.append(i.replace('\\n','').split(' ')[1])\n",
        "    return pos_data[:n_sample]\n",
        "\n",
        "pos_train = read_data(\"train.txt\", 3000)\n",
        "pos_validation = read_data(\"validation.txt\",700)\n",
        "pos_test = read_data(\"test.txt\",3684)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5gUSbrHQ6J2",
        "colab_type": "text"
      },
      "source": [
        "# Features\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH_xBcfvfQp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_sentences = sentence_train_split + sentence_val_split + sentence_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MirHmtHeIcGp",
        "colab_type": "code",
        "outputId": "bb57b43a-254c-4cd5-aa33-77f32042df0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "total_words = []\n",
        "for sentence in total_sentences:\n",
        "    for word in sentence:\n",
        "        total_words.append(word)\n",
        "\n",
        "print(len(total_words))\n",
        "print(len(set(total_words)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "93794\n",
            "13972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ezZdNCrEDCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "total_pos_list = []\n",
        "\n",
        "for line in pos_train + pos_validation + pos_test:\n",
        "    pos_line = []\n",
        "    for each_pos in line:\n",
        "        pos_line.append(each_pos)\n",
        "    total_pos_list.append(pos_line)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWQNSEP8HXtS",
        "colab_type": "code",
        "outputId": "9add89bf-fdcc-4fd2-eb09-67d7feb9d5a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "pos_to_ix = {}\n",
        "pos_list = []\n",
        "for line in pos_train + pos_validation + pos_test:\n",
        "    for pos in line:\n",
        "        if pos not in pos_to_ix:\n",
        "            pos_to_ix[pos] = len(pos_to_ix)\n",
        "pos_list = sorted(list(pos_to_ix.keys()))\n",
        "\n",
        "pos_list_length = len(pos_list)\n",
        "print(pos_list_length)\n",
        "print(pos_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45\n",
            "['\"', '$', \"''\", '(', ')', ',', '-X-', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My7OPDfWVvnk",
        "colab_type": "code",
        "outputId": "50bea32c-a5bf-40e9-ae08-ede41f408b11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# key is the pos tag, value is the one hot expression\n",
        "onehotpos = {}\n",
        "\n",
        "for i in range(pos_list_length):   \n",
        "    onehot = np.eye(1, pos_list_length, i, dtype = int)\n",
        "    onehotpos[pos_list[i]] = list(onehot[0])\n",
        "\n",
        "print(onehotpos)\n",
        "\n",
        "def get_onehotpos(pos_sentence):\n",
        "    pos_list = []\n",
        "    for each_pos in pos_sentence:\n",
        "        onehot_pos = onehotpos[each_pos]\n",
        "        pos_list.append(onehot_pos)\n",
        "    return pos_list"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'\"': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], '$': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \"''\": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], '(': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], ')': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], ',': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], '-X-': [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], '.': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], ':': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'CC': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'CD': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'DT': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'EX': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'FW': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'IN': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'JJ': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'JJR': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'JJS': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'LS': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'MD': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'NN': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'NNP': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'NNPS': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'NNS': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'PDT': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'POS': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'PRP': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'PRP$': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'RB': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'RBR': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'RBS': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'RP': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'SYM': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'TO': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'UH': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'VB': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'VBD': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], 'VBG': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], 'VBN': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], 'VBP': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], 'VBZ': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], 'WDT': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], 'WP': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], 'WP$': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], 'WRB': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r14mferpb4dD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a direction, the key is sentence id, the value is the sentence's one-hot pos list\n",
        "sentence_to_pos = {}\n",
        "total_length = len(sentence_train_split) + len(sentence_val_split) + len(sentence_test_split)\n",
        "for i in range(total_length):\n",
        "    sentence_to_pos[i] = get_onehotpos(total_pos_list[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Sf0ks6vBHqI",
        "colab_type": "code",
        "outputId": "41a5008d-90e9-4856-9c1a-2fff0c464725",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "for i in range(5):\n",
        "    print(sentence_to_pos[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm1NsLQ3qlBP",
        "colab_type": "text"
      },
      "source": [
        "## TF-IDF\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0XY3R9ePJOY",
        "colab_type": "code",
        "outputId": "16df6182-1ac1-40e4-eebb-867e84ade6ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "DF = {}\n",
        "\n",
        "for each_sentence in total_sentences:\n",
        "    for term in np.unique(each_sentence):\n",
        "        try:\n",
        "            DF[term] +=1\n",
        "        except:\n",
        "            DF[term] =1\n",
        "\n",
        "print(len(DF))\n",
        "\n",
        "\n",
        "tf_idf = {}\n",
        "N = total_length\n",
        "print(N)\n",
        "\n",
        "for i in range(N):\n",
        "    counter = Counter(total_sentences[i])\n",
        "    total_num_words = len(total_sentences[i])   \n",
        "    # the tfidf of all words in a sentence\n",
        "    each_sentence_tfidf = []\n",
        "    \n",
        "    for term in total_sentences[i]:\n",
        "        tf = counter[term]/total_num_words\n",
        "        df = DF[term]\n",
        "        idf = math.log(N/(df+1))+1\n",
        "        each_sentence_tfidf.append(tf*idf)\n",
        "    # Create a dictionary, the key is the sentence, the value is the sentence's TF-IDF list\n",
        "    tf_idf[i] = each_sentence_tfidf\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13972\n",
            "7384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0WnqcLhCxNh",
        "colab_type": "code",
        "outputId": "8702b93e-95e1-4cd0-94da-c511d1595769",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "for i in range(5):\n",
        "    print(tf_idf[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3.7867733572317377]\n",
            "[0.7736257552240274, 0.9787176097238419, 0.6639161270509292, 0.8343528337093685, 0.31869858164379405, 1.023769288402527, 0.675381042188177, 0.9467529350069775, 0.20141025902330312]\n",
            "[3.8549230994232344, 3.711082063197344]\n",
            "[3.236541785848771, 2.568193075858512]\n",
            "[0.07835476022474784, 0.2026143126564531, 0.23388996727621744, 0.10974483919929756, 0.10687798888782925, 0.1526398202464369, 0.14205412928464836, 0.2936152829171526, 0.13366389695153158, 0.19917483811527875, 0.28402588050209326, 0.2868287234794147, 0.25348285610628807, 0.2868287234794147, 0.2936152829171526, 0.2026143126564531, 0.28402588050209326, 0.22163247460537353, 0.2936152829171526, 0.28402588050209326, 0.24473738062403838, 0.2609209744834284, 0.25348285610628807, 0.2303779500876232, 0.22163247460537353, 0.15312882657136337, 0.28402588050209326, 0.2868287234794147, 0.24226711488558098, 0.06042307770699094]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFq1dlZnO9mi",
        "colab_type": "code",
        "outputId": "c8588da4-9683-45a4-adab-43c15271ffef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "for i in range(5):\n",
        "    print(len(sentence_to_pos[i]), len(tf_idf[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 1\n",
            "9 9\n",
            "2 2\n",
            "2 2\n",
            "30 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3DcyEtJ93fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_feature(from_feature_index, to_feature_index):\n",
        "    feature_list = []\n",
        "    for i in range(from_feature_index, to_feature_index+1): \n",
        "        sentence_feature_list = []\n",
        "        for j in range(len(sentence_to_pos[i])): \n",
        "            sentence_feature_list.append(sentence_to_pos[i][j]+[tf_idf[i][j]])\n",
        "        \n",
        "        feature_list.append(sentence_feature_list)\n",
        "\n",
        "    return feature_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF0Tv48EMi__",
        "colab_type": "code",
        "outputId": "cd865b0c-9f1d-43b2-ed9a-09630942cec2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "print(get_feature(0,5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3.7867733572317377]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.7736257552240274], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0.9787176097238419], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.6639161270509292], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.8343528337093685], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.31869858164379405], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.023769288402527], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.675381042188177], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.9467529350069775], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.20141025902330312]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3.8549230994232344], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3.711082063197344]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3.236541785848771], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2.568193075858512]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.07835476022474784], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2026143126564531], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.23388996727621744], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0.10974483919929756], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.10687798888782925], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1526398202464369], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.14205412928464836], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0.2936152829171526], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.13366389695153158], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.19917483811527875], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.28402588050209326], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2868287234794147], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.25348285610628807], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2868287234794147], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2936152829171526], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2026143126564531], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.28402588050209326], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.22163247460537353], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2936152829171526], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0.28402588050209326], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.24473738062403838], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2609209744834284], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.25348285610628807], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2303779500876232], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.22163247460537353], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.15312882657136337], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0.28402588050209326], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2868287234794147], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.24226711488558098], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.06042307770699094]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.16596441423822358], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.21886181195549406], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.261784235708214], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.09252539467077893], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1516543746285442], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.19607836708689008], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.20986688369421072], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.21886181195549406], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2525041688549307], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.24530598978027876], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2841438221778896], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2841438221778896], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0.10620468309609442], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.10343031182693152], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.18074211880575866], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.24530598978027876], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.20398554315247025], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.261784235708214], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2841438221778896], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.13360902172251107], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.22460102571020152], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.18697409393578635], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1844327752953633], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.19031411583710375], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.21448303994068407], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1516543746285442], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2568116331331411], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2748637553246064], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0.12397695379016387], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.29722334179428206], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.05847394616805574]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7og4EnAq8aG",
        "colab_type": "text"
      },
      "source": [
        "## Distribution Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO-vnejvEDZ7",
        "colab_type": "code",
        "outputId": "61c8eb58-081e-4161-9f91-51ba6c59da39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sentence_lengths = []\n",
        "for each_sentence in total_sentences:\n",
        "    length = len(each_sentence)\n",
        "    sentence_lengths.append(length)\n",
        "\n",
        "total_sentence_num = len(sentence_lengths)\n",
        "largest_length = max(sentence_lengths)\n",
        "mean_length = np.mean(sentence_lengths)\n",
        "counts_length = np.bincount(sentence_lengths)\n",
        "counts_length = np.argmax(counts_length)\n",
        "length_less_than_counts = 0\n",
        "length_less_than_mean = 0 \n",
        "length_less_than_15 = 0\n",
        "length_less_than_20 = 0\n",
        "length_less_than_25 = 0\n",
        "length_less_than_30 = 0\n",
        "\n",
        "for length in sentence_lengths:\n",
        "  if length < counts_length:\n",
        "    length_less_than_counts += 1\n",
        "  if length < mean_length:\n",
        "    length_less_than_mean += 1\n",
        "  if length < 15:\n",
        "    length_less_than_15 += 1\n",
        "  if length < 20:\n",
        "    length_less_than_20 += 1\n",
        "  if length < 25:\n",
        "    length_less_than_25 += 1\n",
        "  if length < 30:\n",
        "    length_less_than_30 += 1\n",
        "print(\"Total number of sentences: \"+ str(total_sentence_num)+\" Largest length of the sentence: \"+ str(largest_length))\n",
        "x_lengths = [counts_length, mean_length, 15, 20, 25, 30]\n",
        "y_rate = [length_less_than_counts/total_sentence_num, length_less_than_mean/total_sentence_num, length_less_than_15/total_sentence_num, length_less_than_20/total_sentence_num, length_less_than_25/total_sentence_num, length_less_than_30/total_sentence_num]\n",
        "print(y_rate)\n",
        "plt.plot(x_lengths, y_rate, 'g', label = 'Rate of lengths that are smaller than limits')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of sentences: 7384 Largest length of the sentence: 124\n",
            "[0.340465872156013, 0.6533044420368364, 0.6905471289274107, 0.7588028169014085, 0.8263813651137595, 0.8867822318526544]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jFsK7PUsNox",
        "colab_type": "text"
      },
      "source": [
        "There are 5 kinds of tag in total.\n",
        "\n",
        "'I-ORG': Organization\n",
        "\n",
        "'O': Other\n",
        "\n",
        "'I-LOC': Location \n",
        "\n",
        "'I-PER': Person \n",
        "\n",
        "'I-MISC': Misc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-UMIHEKrgCw",
        "colab_type": "code",
        "outputId": "7b7cccaf-1932-41af-9ee8-852ad477e400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "ner_train_list = []\n",
        "for each_ner in ner_train_split:\n",
        "    for each_tag in each_ner:\n",
        "        ner_train_list.append(each_tag)\n",
        "ner_train_set = list(set(ner_train_list))\n",
        "print(len(ner_train_list))\n",
        "print(len(ner_train_set))\n",
        "print(ner_train_set)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39572\n",
            "5\n",
            "['I-LOC', 'O', 'I-ORG', 'I-PER', 'I-MISC']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7chjiqNYkzNK",
        "colab_type": "code",
        "outputId": "17d04841-c179-4a16-a6b2-00b5c10292e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "ner_val_list = []\n",
        "for each_ner in ner_val_split:\n",
        "    for each_tag in each_ner:\n",
        "        ner_val_list.append(each_tag)\n",
        "ner_val_set = list(set(ner_val_list))\n",
        "print(len(ner_val_list))\n",
        "print(len(ner_val_set))\n",
        "print(ner_val_set)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7556\n",
            "5\n",
            "['I-LOC', 'O', 'I-ORG', 'I-PER', 'I-MISC']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDuagDgs1Yj4",
        "colab_type": "code",
        "outputId": "fe44e40b-9b72-45c8-b6b4-47fe452fd758",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# count the number of each tag\n",
        "num_org = 0\n",
        "num_o = 0\n",
        "num_loc = 0\n",
        "num_per = 0\n",
        "num_misc = 0\n",
        "for i in ner_train_list:\n",
        "    if i=='I-ORG':\n",
        "        num_org += 1\n",
        "    if i=='O':\n",
        "        num_o += 1\n",
        "    if i=='I-LOC':\n",
        "        num_loc += 1\n",
        "    if i=='I-PER':\n",
        "        num_per += 1\n",
        "    if i=='I-MISC':\n",
        "        num_misc += 1\n",
        "print(\"I-ORG:  \", num_org)\n",
        "print(\"O:      \", num_o)\n",
        "print(\"I-LOC:  \", num_loc)\n",
        "print(\"I-PER:  \", num_per)\n",
        "print(\"I-MISC: \", num_misc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I-ORG:   1526\n",
            "O:       32137\n",
            "I-LOC:   1994\n",
            "I-PER:   2840\n",
            "I-MISC:  1075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_NXJRv7k__M",
        "colab_type": "code",
        "outputId": "ed152060-f023-47e1-8121-770dccc94862",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# count the number of each tag\n",
        "num_org = 0\n",
        "num_o = 0\n",
        "num_loc = 0\n",
        "num_per = 0\n",
        "num_misc = 0\n",
        "for i in ner_val_list:\n",
        "    if i=='I-ORG':\n",
        "        num_org += 1\n",
        "    if i=='O':\n",
        "        num_o += 1\n",
        "    if i=='I-LOC':\n",
        "        num_loc += 1\n",
        "    if i=='I-PER':\n",
        "        num_per += 1\n",
        "    if i=='I-MISC':\n",
        "        num_misc += 1\n",
        "print(\"I-ORG:  \", num_org)\n",
        "print(\"O:      \", num_o)\n",
        "print(\"I-LOC:  \", num_loc)\n",
        "print(\"I-PER:  \", num_per)\n",
        "print(\"I-MISC: \", num_misc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I-ORG:   285\n",
            "O:       5790\n",
            "I-LOC:   419\n",
            "I-PER:   875\n",
            "I-MISC:  187\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP9s9J-lDKUC",
        "colab_type": "text"
      },
      "source": [
        "In the data, you can see the different types of entities: \n",
        "* geo = Geographical Entity\n",
        "* org = Organization\n",
        "* per = Person\n",
        "* gpe = Geopolitical Entity\n",
        "* tim = Time indicator\n",
        "* art = Artifact\n",
        "* eve = Event\n",
        "* nat = Natural Phenomenon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfakfv0CRwAB",
        "colab_type": "text"
      },
      "source": [
        "## Data Preprocessing\n",
        "There are too many NaN values in Sentence # column, fill NaN by preceding values.\n",
        "We have 47595 sentences that contain 35172 unique words and tagged by 17 tags."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pcu5O3OlEdPA",
        "colab_type": "text"
      },
      "source": [
        "We will now train a CRF model for named entity recognition using sklearn-crfsuite on our dataset. As mentioned before, MEMM or CRF is often used for labeling or parsing of sequential data for named entity recognition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EsvF8uXSgy5",
        "colab_type": "code",
        "outputId": "4ffbe761-18ff-4438-dfc5-03426f8962e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!pip install sklearn_crfsuite"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn_crfsuite in /usr/local/lib/python3.6/dist-packages (0.3.6)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (0.9.7)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (0.8.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (1.12.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4F991p-IqNS",
        "colab_type": "text"
      },
      "source": [
        "##Conditional random fields (CRF)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-Xcf07iR79l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics\n",
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQPELELtSmYo",
        "colab_type": "code",
        "outputId": "095d0c1c-f5bd-4fbc-f128-e1b4d9141c62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "#Retrieving sentences with their tags in zip\n",
        "zip_sentences_train = []\n",
        "for line in range(len(sentence_train_split)):\n",
        "    combine_sentence = []\n",
        "    for each_word, each_tag in zip(sentence_train_split[line], ner_train_split[line]):\n",
        "        combine_sentence.append((each_word, each_tag))\n",
        "    zip_sentences_train.append(combine_sentence)\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(zip_sentences_train[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('-docstart-', 'O')]\n",
            "[('eu', 'I-ORG'), ('rejects', 'O'), ('german', 'I-MISC'), ('call', 'O'), ('to', 'O'), ('boycott', 'O'), ('british', 'I-MISC'), ('lamb', 'O'), ('.', 'O')]\n",
            "[('peter', 'I-PER'), ('blackburn', 'I-PER')]\n",
            "[('brussels', 'I-LOC'), ('1996-08-22', 'O')]\n",
            "[('the', 'O'), ('european', 'I-ORG'), ('commission', 'I-ORG'), ('said', 'O'), ('on', 'O'), ('thursday', 'O'), ('it', 'O'), ('disagreed', 'O'), ('with', 'O'), ('german', 'I-MISC'), ('advice', 'O'), ('to', 'O'), ('consumers', 'O'), ('to', 'O'), ('shun', 'O'), ('british', 'I-MISC'), ('lamb', 'O'), ('until', 'O'), ('scientists', 'O'), ('determine', 'O'), ('whether', 'O'), ('mad', 'O'), ('cow', 'O'), ('disease', 'O'), ('can', 'O'), ('be', 'O'), ('transmitted', 'O'), ('to', 'O'), ('sheep', 'O'), ('.', 'O')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCJIH2emlP1y",
        "colab_type": "code",
        "outputId": "86973ed5-bec9-44c3-c538-6e165f1f0cfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "#Retrieving sentences with their tags in zip\n",
        "zip_sentences_val = []\n",
        "for line in range(len(sentence_val_split)):\n",
        "    combine_sentence_val = []\n",
        "    for each_word, each_tag in zip(sentence_val_split[line], ner_val_split[line]):\n",
        "        combine_sentence_val.append((each_word, each_tag))\n",
        "    zip_sentences_val.append(combine_sentence_val)\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(zip_sentences_val[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('-docstart-', 'O')]\n",
            "[('cricket', 'O'), ('-', 'O'), ('leicestershire', 'I-ORG'), ('take', 'O'), ('over', 'O'), ('at', 'O'), ('top', 'O'), ('after', 'O'), ('innings', 'O'), ('victory', 'O'), ('.', 'O')]\n",
            "[('london', 'I-LOC'), ('1996-08-30', 'O')]\n",
            "[('west', 'I-MISC'), ('indian', 'I-MISC'), ('all-rounder', 'O'), ('phil', 'I-PER'), ('simmons', 'I-PER'), ('took', 'O'), ('four', 'O'), ('for', 'O'), ('38', 'O'), ('on', 'O'), ('friday', 'O'), ('as', 'O'), ('leicestershire', 'I-ORG'), ('beat', 'O'), ('somerset', 'I-ORG'), ('by', 'O'), ('an', 'O'), ('innings', 'O'), ('and', 'O'), ('39', 'O'), ('runs', 'O'), ('in', 'O'), ('two', 'O'), ('days', 'O'), ('to', 'O'), ('take', 'O'), ('over', 'O'), ('at', 'O'), ('the', 'O'), ('head', 'O'), ('of', 'O'), ('the', 'O'), ('county', 'O'), ('championship', 'O'), ('.', 'O')]\n",
            "[('their', 'O'), ('stay', 'O'), ('on', 'O'), ('top', 'O'), (',', 'O'), ('though', 'O'), (',', 'O'), ('may', 'O'), ('be', 'O'), ('short-lived', 'O'), ('as', 'O'), ('title', 'O'), ('rivals', 'O'), ('essex', 'I-ORG'), (',', 'O'), ('derbyshire', 'I-ORG'), ('and', 'O'), ('surrey', 'I-ORG'), ('all', 'O'), ('closed', 'O'), ('in', 'O'), ('on', 'O'), ('victory', 'O'), ('while', 'O'), ('kent', 'I-ORG'), ('made', 'O'), ('up', 'O'), ('for', 'O'), ('lost', 'O'), ('time', 'O'), ('in', 'O'), ('their', 'O'), ('rain-affected', 'O'), ('match', 'O'), ('against', 'O'), ('nottinghamshire', 'I-ORG'), ('.', 'O')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alfVxIWIFEth",
        "colab_type": "text"
      },
      "source": [
        "We extract more features (word parts, simplified POS tags, lower/title/upper flags, features of nearby words) and convert them to sklearn-crfsuite formateach sentence should be converted to a list of dicts. The following code were taken from [sklearn-crfsuites official site](https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bbaee897Sqoa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    # postag = sent[i][1]\n",
        "    # a few features: upper? lower? title? \n",
        "    features = {\n",
        "        'bias': 1.0, \n",
        "        'word.lower()': word.lower(), \n",
        "        'word[-3:]': word[-3:], # last three letters of the word\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(), \n",
        "        'word.istitle()': word.istitle(), # is it a title?\n",
        "        'word.isdigit()': word.isdigit(), # is it a digit?\n",
        "       \n",
        "    }\n",
        "    # features of the previous word\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        # postag1 = sent[i-1][1]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "          \n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "    # features of the next word\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "       \n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "            \n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "    return features\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "def sent2labels(sent):\n",
        "    return [label for token, label in sent]\n",
        "def sent2tokens(sent):\n",
        "    return [token for token, label in sent]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME9Xu722SzYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "features_train = [sent2features(s) for s in zip_sentences_train]\n",
        "labels_train = [sent2labels(s) for s in zip_sentences_train]\n",
        "\n",
        "features_val = [sent2features(s) for s in zip_sentences_val]\n",
        "labels_val = [sent2labels(s) for s in zip_sentences_val]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3g66D_kMlCp",
        "colab_type": "code",
        "outputId": "4bce6724-f851-4c55-b23f-d4669815fbfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "for i in range(5):\n",
        "     print(features_train[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'bias': 1.0, 'word.lower()': '-docstart-', 'word[-3:]': 'rt-', 'word[-2:]': 't-', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'BOS': True, 'EOS': True}]\n",
            "[{'bias': 1.0, 'word.lower()': 'eu', 'word[-3:]': 'eu', 'word[-2:]': 'eu', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'BOS': True, '+1:word.lower()': 'rejects', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'rejects', 'word[-3:]': 'cts', 'word[-2:]': 'ts', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'eu', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'german', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'german', 'word[-3:]': 'man', 'word[-2:]': 'an', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'rejects', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'call', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'call', 'word[-3:]': 'all', 'word[-2:]': 'll', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'german', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'to', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'to', 'word[-3:]': 'to', 'word[-2:]': 'to', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'call', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'boycott', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'boycott', 'word[-3:]': 'ott', 'word[-2:]': 'tt', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'to', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'british', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'british', 'word[-3:]': 'ish', 'word[-2:]': 'sh', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'boycott', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'lamb', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'lamb', 'word[-3:]': 'amb', 'word[-2:]': 'mb', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'british', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': '.', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': '.', 'word[-3:]': '.', 'word[-2:]': '.', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'lamb', '-1:word.istitle()': False, '-1:word.isupper()': False, 'EOS': True}]\n",
            "[{'bias': 1.0, 'word.lower()': 'peter', 'word[-3:]': 'ter', 'word[-2:]': 'er', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'BOS': True, '+1:word.lower()': 'blackburn', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'blackburn', 'word[-3:]': 'urn', 'word[-2:]': 'rn', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'peter', '-1:word.istitle()': False, '-1:word.isupper()': False, 'EOS': True}]\n",
            "[{'bias': 1.0, 'word.lower()': 'brussels', 'word[-3:]': 'els', 'word[-2:]': 'ls', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'BOS': True, '+1:word.lower()': '1996-08-22', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': '1996-08-22', 'word[-3:]': '-22', 'word[-2:]': '22', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'brussels', '-1:word.istitle()': False, '-1:word.isupper()': False, 'EOS': True}]\n",
            "[{'bias': 1.0, 'word.lower()': 'the', 'word[-3:]': 'the', 'word[-2:]': 'he', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'BOS': True, '+1:word.lower()': 'european', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'european', 'word[-3:]': 'ean', 'word[-2:]': 'an', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'the', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'commission', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'commission', 'word[-3:]': 'ion', 'word[-2:]': 'on', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'european', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'said', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'said', 'word[-3:]': 'aid', 'word[-2:]': 'id', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'commission', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'on', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'on', 'word[-3:]': 'on', 'word[-2:]': 'on', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'said', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'thursday', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'thursday', 'word[-3:]': 'day', 'word[-2:]': 'ay', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'on', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'it', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'it', 'word[-3:]': 'it', 'word[-2:]': 'it', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'thursday', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'disagreed', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'disagreed', 'word[-3:]': 'eed', 'word[-2:]': 'ed', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'it', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'with', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'with', 'word[-3:]': 'ith', 'word[-2:]': 'th', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'disagreed', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'german', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'german', 'word[-3:]': 'man', 'word[-2:]': 'an', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'with', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'advice', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'advice', 'word[-3:]': 'ice', 'word[-2:]': 'ce', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'german', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'to', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'to', 'word[-3:]': 'to', 'word[-2:]': 'to', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'advice', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'consumers', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'consumers', 'word[-3:]': 'ers', 'word[-2:]': 'rs', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'to', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'to', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'to', 'word[-3:]': 'to', 'word[-2:]': 'to', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'consumers', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'shun', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'shun', 'word[-3:]': 'hun', 'word[-2:]': 'un', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'to', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'british', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'british', 'word[-3:]': 'ish', 'word[-2:]': 'sh', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'shun', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'lamb', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'lamb', 'word[-3:]': 'amb', 'word[-2:]': 'mb', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'british', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'until', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'until', 'word[-3:]': 'til', 'word[-2:]': 'il', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'lamb', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'scientists', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'scientists', 'word[-3:]': 'sts', 'word[-2:]': 'ts', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'until', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'determine', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'determine', 'word[-3:]': 'ine', 'word[-2:]': 'ne', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'scientists', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'whether', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'whether', 'word[-3:]': 'her', 'word[-2:]': 'er', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'determine', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'mad', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'mad', 'word[-3:]': 'mad', 'word[-2:]': 'ad', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'whether', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'cow', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'cow', 'word[-3:]': 'cow', 'word[-2:]': 'ow', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'mad', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'disease', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'disease', 'word[-3:]': 'ase', 'word[-2:]': 'se', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'cow', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'can', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'can', 'word[-3:]': 'can', 'word[-2:]': 'an', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'disease', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'be', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'be', 'word[-3:]': 'be', 'word[-2:]': 'be', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'can', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'transmitted', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'transmitted', 'word[-3:]': 'ted', 'word[-2:]': 'ed', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'be', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'to', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'to', 'word[-3:]': 'to', 'word[-2:]': 'to', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'transmitted', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': 'sheep', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': 'sheep', 'word[-3:]': 'eep', 'word[-2:]': 'ep', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'to', '-1:word.istitle()': False, '-1:word.isupper()': False, '+1:word.lower()': '.', '+1:word.istitle()': False, '+1:word.isupper()': False}, {'bias': 1.0, 'word.lower()': '.', 'word[-3:]': '.', 'word[-2:]': '.', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, '-1:word.lower()': 'sheep', '-1:word.istitle()': False, '-1:word.isupper()': False, 'EOS': True}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F2kJfNAS1N2",
        "colab_type": "code",
        "outputId": "75491dbf-f749-4345-9db7-112d42eb5c5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "# train a CRF model for named entity recognition using sklearn-crfsuite \n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "crf.fit(features_train, labels_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRF(algorithm='lbfgs', all_possible_states=None, all_possible_transitions=True,\n",
              "    averaging=None, c=None, c1=0.1, c2=0.1, calibration_candidates=None,\n",
              "    calibration_eta=None, calibration_max_trials=None, calibration_rate=None,\n",
              "    calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
              "    gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=100,\n",
              "    max_linesearch=None, min_freq=None, model_filename=None, num_memories=None,\n",
              "    pa_type=None, period=None, trainer_cls=None, variance=None, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnxUCHw5FXGt",
        "colab_type": "text"
      },
      "source": [
        "Because tag O (outside) is the most common tag and it will make our results look much better than they actual are. So we remove tag O when we evaluate classification metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGFs5K6EVC0R",
        "colab_type": "text"
      },
      "source": [
        "B: begining of ... \n",
        "I: identity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSnMwxl4UVPN",
        "colab_type": "code",
        "outputId": "ea7890f8-087c-4239-d2cd-dfe565655c31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ner_train_set.remove('O')\n",
        "classes = ner_train_set\n",
        "print(classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I-LOC', 'I-ORG', 'I-PER', 'I-MISC']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C9jdSefS4id",
        "colab_type": "code",
        "outputId": "3eef968b-0baa-4263-c729-f9b0069dc6d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "#evaluation\n",
        "y_pred = crf.predict(features_val)\n",
        "print(metrics.flat_classification_report(labels_val, y_pred, labels=classes))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       I-LOC       0.89      0.85      0.87       419\n",
            "       I-ORG       0.85      0.58      0.69       285\n",
            "       I-PER       0.93      0.80      0.86       875\n",
            "      I-MISC       0.83      0.63      0.72       187\n",
            "\n",
            "   micro avg       0.90      0.76      0.82      1766\n",
            "   macro avg       0.88      0.72      0.78      1766\n",
            "weighted avg       0.90      0.76      0.82      1766\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3oWq_PnK8wk",
        "colab_type": "text"
      },
      "source": [
        "The following shows what our classifier learned. It is very likely that the beginning of a geographical entity (B-geo) will be followed by a token inside geographical entity (I-geo), but transitions to inside of an organization name (I-org) from tokens with other labels are penalized hugely."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40His-7UKyvp",
        "colab_type": "code",
        "outputId": "3bef6176-c0dd-4943-a070-fbfa156636fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "def print_transitions(trans_features):\n",
        "    for (label_from, label_to), weight in trans_features:\n",
        "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
        "print(\"Top likely transitions:\")\n",
        "print_transitions(Counter(crf.transition_features_).most_common(5))\n",
        "print(\"\\nTop unlikely transitions:\")\n",
        "print_transitions(Counter(crf.transition_features_).most_common()[-5:])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top likely transitions:\n",
            "I-PER  -> I-PER   3.403927\n",
            "I-ORG  -> I-ORG   3.384090\n",
            "I-MISC -> I-MISC  2.828140\n",
            "I-LOC  -> I-LOC   2.064296\n",
            "O      -> O       0.776679\n",
            "\n",
            "Top unlikely transitions:\n",
            "I-PER  -> I-LOC   -2.677780\n",
            "I-PER  -> I-ORG   -2.689004\n",
            "I-LOC  -> I-PER   -3.410616\n",
            "I-ORG  -> I-LOC   -3.463856\n",
            "I-ORG  -> I-PER   -3.736825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQbtE0AYpvtX",
        "colab_type": "text"
      },
      "source": [
        "## Bi-LSTM CRF model\n",
        "\n",
        "Now we will apply the Bi-LSTM CRF model we just learned to CoNLL 2003 NER dataset using the pretrained glove embeddings. Please go through and complete the [Function for accuracy] section. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cCqGvJchOdR",
        "colab_type": "text"
      },
      "source": [
        "#### Generate word_to_ix and tag_to_ix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MWAK0NV3INF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert each unique word to the index\n",
        "word_to_ix = {}\n",
        "for sentence in total_sentences:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "word_list = list(word_to_ix.keys())\n",
        "\n",
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "tag_to_ix = {START_TAG:0, STOP_TAG:1}\n",
        "for tags in ner_train_split + ner_val_split:\n",
        "    for tag in tags:\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNgyvEADwYZt",
        "colab_type": "code",
        "outputId": "8d8c86ab-e5dd-4e3d-ce51-bfdcf518de2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "print(len(word_list))\n",
        "for i in range(5):\n",
        "    print(word_list[i])\n",
        "    print(list(word_to_ix.values())[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13972\n",
            "-docstart-\n",
            "0\n",
            "eu\n",
            "1\n",
            "rejects\n",
            "2\n",
            "german\n",
            "3\n",
            "call\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tndg_-ttwa6N",
        "colab_type": "code",
        "outputId": "bdbca526-27ba-47ff-bfba-e6f4f9e04b19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tag_to_ix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<START>': 0, '<STOP>': 1, 'O': 2, 'I-ORG': 3, 'I-MISC': 4, 'I-PER': 5, 'I-LOC': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajrekRAbtxL2",
        "colab_type": "code",
        "outputId": "d23aa126-7ac8-45c9-9c64-d164610515d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(word_list), len(tag_to_ix))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13972 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEswz2QjhXBM",
        "colab_type": "text"
      },
      "source": [
        "#### Generate Embedding Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Oz6KVyjsxM9",
        "colab_type": "code",
        "outputId": "ec525f49-b854-4d51-c2f7-4498aaa8f778",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "# change a latest embedding!\n",
        "word_emb_model = api.load(\"glove-wiki-gigaword-50\") \n",
        "# word dimension\n",
        "EMBEDDING_DIM = 50\n",
        "embedding_matrix = []\n",
        "num_except = 0\n",
        "for word in word_list:\n",
        "    try:\n",
        "        embedding_matrix.append(word_emb_model.wv[word])\n",
        "    except:\n",
        "        num_except += 1\n",
        "        embedding_matrix.append([0]*EMBEDDING_DIM) # '-docstart-': 0*50\n",
        "\n",
        "embedding_matrix = np.array(embedding_matrix)\n",
        "embedding_matrix.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13972, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlchZJO8hdXa",
        "colab_type": "text"
      },
      "source": [
        "#### convert dataset into idxs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRs6mouFwEx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_index(data, to_ix):\n",
        "    input_index_list = []\n",
        "    for sent in data:\n",
        "        input_index_list.append([to_ix[w] for w in sent])\n",
        "    return input_index_list\n",
        "\n",
        "# use the index to represent each word in each sentence\n",
        "train_input_index =  to_index(sentence_train_split, word_to_ix)\n",
        "train_output_index = to_index(ner_train_split, tag_to_ix)\n",
        "\n",
        "train_input_feature = get_feature(0, 2999)\n",
        "\n",
        "val_input_index = to_index(sentence_val_split, word_to_ix)\n",
        "val_output_index = to_index(ner_val_split, tag_to_ix)\n",
        "val_input_feature = get_feature(3000, 3699)\n",
        "\n",
        "test_input_index = to_index(sentence_test_split, word_to_ix)\n",
        "test_input_feature = get_feature(3700, 7383)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDEcKXhP_yLV",
        "colab_type": "code",
        "outputId": "cc1bea65-ddc2-4394-be05-e8fba86f4789",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "for i in range(5):\n",
        "    print(train_input_index[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "[10, 11]\n",
            "[12, 13]\n",
            "[14, 15, 16, 17, 18, 19, 20, 21, 22, 3, 23, 5, 24, 5, 25, 7, 8, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 5, 36, 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqtXLVSPFJgl",
        "colab_type": "code",
        "outputId": "d434364e-a008-45b8-eda1-16535ea600f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "for i in range(5):\n",
        "    print(val_input_index[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\n",
            "[2001, 636, 2045, 210, 775, 154, 382, 117, 2090, 1798, 9]\n",
            "[261, 7913]\n",
            "[1001, 5476, 2285, 1693, 2048, 764, 2167, 68, 7914, 18, 1094, 141, 2045, 1758, 2017, 91, 155, 2090, 84, 3717, 2983, 229, 391, 947, 5, 210, 775, 154, 14, 1811, 159, 14, 2002, 2003, 9]\n",
            "[196, 4831, 18, 382, 69, 6020, 69, 1062, 34, 7915, 141, 1971, 388, 2034, 69, 7916, 84, 2070, 412, 1358, 229, 18, 1798, 477, 2037, 3390, 305, 68, 2851, 367, 229, 196, 7917, 1768, 742, 2064, 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igZht_BbxOvw",
        "colab_type": "code",
        "outputId": "5b386e17-7477-4619-ee04-e01537503ed5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "for i in range(5):\n",
        "    print(train_output_index[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2]\n",
            "[3, 2, 4, 2, 2, 2, 4, 2, 2]\n",
            "[5, 5]\n",
            "[6, 2]\n",
            "[2, 3, 3, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXEscWBrhjgb",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5AgRWakkfmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    \n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        \n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        \n",
        "\n",
        "        # self.multihead_attn2 = nn.MultiheadAttention(embedding_dim, 2)\n",
        "        # Use nn.Embedding\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # Use the embedding matrix as the initial weights of nn.Embedding\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "\n",
        "        # self.multiheadattn1 = nn.MultiheadAttention(embedding_dim, 2)\n",
        "\n",
        "        self.lstm_1 = nn.LSTM(100, hidden_dim // 2, num_layers=1, bidirectional=True)\n",
        "        self.lstm_2 = nn.LSTM(100, hidden_dim // 2, num_layers=1, bidirectional=True)\n",
        "        self.lstm_3 = nn.LSTM(96, hidden_dim // 2, num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "      \n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n",
        "    \n",
        "    # CRF: use the transition matrix to train our model\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of the previous tag\n",
        "                emit_score = feat[next_tag].view(1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    \n",
        "\n",
        "    def cal_self_attention(self, hidden, method):\n",
        "        # 1st type of calculation of the attention: Dot Product\n",
        "        if method == 'Dot Product':\n",
        "            a = torch.bmm(hidden, hidden.permute(0,2,1)) # [1, seq, 50] * [1,50,seq] = [1,seq, seq]\n",
        "            # print(\"hidden1 \"+str(hidden1.shape))\n",
        "            # print(\"hidden2 \"+str(hidden2.shape))\n",
        "            # print(\"a shape \" + str(a.shape)) \n",
        "          \n",
        "            attn_weights = F.softmax(a, dim=-1)\n",
        "            # print(\"attn_weights\" + str(attn_weights.shape)) # [1, seq, 50]\n",
        "            # attn_weights = F.softmax(torch.bmm(hidden1, hidden2), dim=-1)\n",
        "            # print(\"hidden2.u\" + str((hidden2).shape))\n",
        "            attn_output = torch.bmm(attn_weights, hidden) \n",
        "            concat_output = torch.cat((attn_output, hidden), -1)\n",
        "            # print(concat_output,concat_output.size())\n",
        "\n",
        "        # 2nd type of calculation of the attention: Scaled Dot Product\n",
        "        elif method == 'Scale Dot Product':\n",
        "            a = torch.bmm(hidden, hidden.permute(0,2,1))\n",
        "            attn_weights = F.softmax((1/np.sqrt(self.hidden_dim))*a, dim = -1)\n",
        "            attn_output = torch.bmm(attn_weights, hidden) \n",
        "            concat_output = torch.cat((attn_output, hidden), -1)\n",
        "\n",
        "        return concat_output\n",
        "\n",
        "\n",
        "    def _get_lstm_features(self, sentence, features):\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "\n",
        "        # 96\n",
        "        features = features.view(len(features),1,-1)\n",
        "        # concat the word embedding with the word features\n",
        "        word_concat = torch.cat((embeds, features), dim = 2) # [1,1,96]\n",
        "\n",
        "     \n",
        "        lstm_out_summary, (h_n,c_n) = self.lstm_3(word_concat, self.hidden) \n",
        "        \n",
        "        attn_output1 = self.cal_self_attention(lstm_out_summary, 'Dot Product')\n",
        "        \n",
        "\n",
        "        lstm_out_1, self.hidden = self.lstm_1(attn_output1, self.hidden) \n",
        "        \n",
        "        attn_output2 = self.cal_self_attention(lstm_out_1, 'Dot Product')\n",
        "\n",
        "        lstm_out_2, _ = self.lstm_2(attn_output2)     \n",
        "\n",
        "        \n",
        "        # change from 3 dimensions to 2 dimensions\n",
        "        lstm_out = lstm_out_2.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    ''' Calculate the score for the viterbi decoding'''\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, features, tags):\n",
        "        feats = self._get_lstm_features(sentence, features)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    # sentence is a index expression of the words in the sentence \n",
        "    def forward(self, sentence, features):  \n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence, features)\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUNHEV1kiDKt",
        "colab_type": "text"
      },
      "source": [
        "#### Function for accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Moqs-zwboIn",
        "colab_type": "text"
      },
      "source": [
        "Please complete the cal_acc function that generates the model predictions using the input data and calculates the accuracy by comparing the model predictions with the ground truth labels. You can refer to the [Train the model] section regarding what the inputs and outputs are and how it will be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJmu0oSsjLBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "def cal_acc(model, input_index, input_feature, output_index):\n",
        "    ground_truth = []\n",
        "    predicted = []\n",
        "    for i in range(len(input_index)):\n",
        "      input_index_tensor = torch.tensor(input_index[i]).to(device)\n",
        "      input_feature_float = torch.from_numpy(np.array(input_feature[i])).float().to(device)\n",
        "      # add elements of tuple to list\n",
        "      ground_truth.extend(output_index[i])\n",
        "      _, outputs = model(input_index_tensor, input_feature_float)\n",
        "      predicted.extend(outputs)\n",
        "      accuracy = accuracy_score(predicted, ground_truth)\n",
        "      \n",
        "    return ground_truth, predicted, accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_82UaXEOhoQQ",
        "colab_type": "text"
      },
      "source": [
        "#### Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyuzZ_et6FD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "HIDDEN_DIM = 50\n",
        "\n",
        "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9UiokVOjPUn",
        "colab_type": "text"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0CMFVSwlLru",
        "colab_type": "code",
        "outputId": "27795f32-389a-47f6-edb6-87227560e481",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "\"\"\"Each epoch will take about 370s\"\"\"\n",
        "\n",
        "import datetime\n",
        "\n",
        "epoch_list = []\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "val_loss_list = []\n",
        "val_acc_list = []\n",
        "for epoch in range(20): \n",
        "    epoch_list.append(epoch) \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "    model.train()\n",
        "    # i: index; idxs: word\n",
        "    for i, idxs in enumerate(train_input_index):\n",
        "        tags_index = train_output_index[i]\n",
        "\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "  \n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        sentence_features_train = torch.from_numpy(np.array(train_input_feature[i])).float().to(device)\n",
        "        \n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        # Use the forward pass\n",
        "        loss = model.neg_log_likelihood(sentence_in, sentence_features_train, targets)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    train_loss_list.append(train_loss)\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    _, _, train_acc = cal_acc(model, train_input_index, train_input_feature, train_output_index)\n",
        "    _, _, val_acc = cal_acc(model,val_input_index, val_input_feature, val_output_index)\n",
        "\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    val_loss = 0\n",
        "    # i: index; idxs: word\n",
        "    for i, idxs in enumerate(val_input_index):\n",
        "        tags_index = val_output_index[i]\n",
        "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "        sentence_features_val = torch.from_numpy(np.array(val_input_feature[i])).float().to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        loss = model.neg_log_likelihood(sentence_in, sentence_features_val, targets)\n",
        "        val_loss+=loss.item()\n",
        "        \n",
        "    val_loss_list.append(val_loss)\n",
        "    val_acc_list.append(val_acc)\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 16624.83, train acc: 0.9386, val loss: 1810.23, val acc: 0.9217, time: 383.44s\n",
            "Epoch:2, Training loss: 4975.51, train acc: 0.9635, val loss: 1650.60, val acc: 0.9185, time: 377.96s\n",
            "Epoch:3, Training loss: 2507.17, train acc: 0.9821, val loss: 1416.78, val acc: 0.9471, time: 375.36s\n",
            "Epoch:4, Training loss: 1598.60, train acc: 0.9801, val loss: 1559.54, val acc: 0.9399, time: 371.21s\n",
            "Epoch:5, Training loss: 1201.75, train acc: 0.9819, val loss: 1764.40, val acc: 0.9309, time: 375.82s\n",
            "Epoch:6, Training loss: 967.94, train acc: 0.9919, val loss: 1693.12, val acc: 0.9461, time: 374.83s\n",
            "Epoch:7, Training loss: 922.65, train acc: 0.9932, val loss: 1538.62, val acc: 0.9506, time: 374.45s\n",
            "Epoch:8, Training loss: 752.60, train acc: 0.9935, val loss: 1445.59, val acc: 0.9537, time: 374.28s\n",
            "Epoch:9, Training loss: 588.75, train acc: 0.9946, val loss: 1655.50, val acc: 0.9569, time: 378.90s\n",
            "Epoch:10, Training loss: 573.59, train acc: 0.9956, val loss: 1579.03, val acc: 0.9575, time: 378.58s\n",
            "Epoch:11, Training loss: 555.54, train acc: 0.9957, val loss: 1449.20, val acc: 0.9571, time: 379.24s\n",
            "Epoch:12, Training loss: 515.55, train acc: 0.9940, val loss: 1549.13, val acc: 0.9517, time: 376.14s\n",
            "Epoch:13, Training loss: 460.35, train acc: 0.9956, val loss: 1655.71, val acc: 0.9563, time: 375.12s\n",
            "Epoch:14, Training loss: 430.47, train acc: 0.9957, val loss: 1540.36, val acc: 0.9575, time: 372.68s\n",
            "Epoch:15, Training loss: 540.06, train acc: 0.9955, val loss: 1611.31, val acc: 0.9524, time: 376.57s\n",
            "Epoch:16, Training loss: 467.89, train acc: 0.9952, val loss: 1590.23, val acc: 0.9494, time: 378.60s\n",
            "Epoch:17, Training loss: 399.44, train acc: 0.9961, val loss: 1508.54, val acc: 0.9580, time: 378.92s\n",
            "Epoch:18, Training loss: 373.37, train acc: 0.9876, val loss: 1737.11, val acc: 0.9479, time: 380.04s\n",
            "Epoch:19, Training loss: 526.19, train acc: 0.9968, val loss: 1451.60, val acc: 0.9553, time: 374.93s\n",
            "Epoch:20, Training loss: 436.81, train acc: 0.9975, val loss: 1394.23, val acc: 0.9566, time: 367.04s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj7ANbv_jSzI",
        "colab_type": "text"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIh-mWrvi6Pw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_true, y_pred,_ = cal_acc(model, val_input_index, val_input_feature, val_output_index)\n",
        "\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAnJVsyPq5kR",
        "colab_type": "code",
        "outputId": "a3af623b-3e04-483a-a030-6d6e25c9f759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       I-LOC     0.8140    0.9189    0.8632       419\n",
            "      I-MISC     0.8263    0.7380    0.7797       187\n",
            "       I-ORG     0.7660    0.7123    0.7382       285\n",
            "       I-PER     0.9313    0.8983    0.9145       875\n",
            "           O     0.9855    0.9884    0.9870      5790\n",
            "\n",
            "    accuracy                         0.9575      7556\n",
            "   macro avg     0.8646    0.8512    0.8565      7556\n",
            "weighted avg     0.9575    0.9575    0.9572      7556\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao_KMytuHgOB",
        "colab_type": "code",
        "outputId": "6cbbde44-358f-4ada-9e13-55592f23ffa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "torch.save(model, 'Bilstm_crf_model11.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type BiLSTM_CRF. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vULrL_HFGrgt",
        "colab_type": "code",
        "outputId": "413371a5-5f05-4ead-8b63-f6a63fd9add5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "for i in range(5):\n",
        "    print(y_pred_decode[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O\n",
            "O\n",
            "O\n",
            "I-LOC\n",
            "O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84Vihry_Y3lP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def leaderboard(model, input_index, input_feature):\n",
        "    predicted = []\n",
        "    for i in range(len(input_index)):\n",
        "      input_index_tensor = torch.tensor(input_index[i]).to(device)\n",
        "      input_feature_float = torch.from_numpy(np.array(input_feature[i])).float().to(device)\n",
        "      # add elements of tuple to list\n",
        "      \n",
        "      _, outputs = model(input_index_tensor, input_feature_float)\n",
        "      predicted.extend(outputs)\n",
        "      \n",
        "    return predicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0Y0fX78Ysm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred = leaderboard(model, test_input_index, test_input_feature)\n",
        "\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "\n",
        "test_pred_decode = decode_output(test_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7m0Mv2xaiq-",
        "colab_type": "code",
        "outputId": "8725a978-9c66-4a30-d614-5dc6c54f29ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "for i in range(5):\n",
        "    print(test_pred_decode[i])\n",
        "print(len(test_pred_decode))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O\n",
            "O\n",
            "O\n",
            "I-LOC\n",
            "O\n",
            "46666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2xqOcTPz3iU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "co1 = []\n",
        "co2 = []\n",
        "for i in range(len(test_pred_decode)):\n",
        "    co1.append(i)\n",
        "    co2.append(test_pred_decode[i])\n",
        "leaderboard = pd.DataFrame({'Id':co1, 'Predicted':co2})\n",
        "leaderboard.to_csv(\"leaderboard4_1.csv\", index=False, sep=\",\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXGK44ibbYBp",
        "colab_type": "code",
        "outputId": "ac7e870e-5683-4d0f-fbd5-a123aa03c55c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "da=pd.read_csv(\"leaderboard4_1.csv\")\n",
        "da.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>I-LOC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>I-PER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>I-LOC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id Predicted\n",
              "0   0         O\n",
              "1   1         O\n",
              "2   2         O\n",
              "3   3     I-LOC\n",
              "4   4         O\n",
              "5   5     I-PER\n",
              "6   6         O\n",
              "7   7         O\n",
              "8   8     I-LOC\n",
              "9   9         O"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    }
  ]
}